{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data generation\n",
        "\n",
        "Esta es una forma de generar un dataset sintético con GPT-4 y una prompt.\n",
        "\n",
        "El dataset debe seguir el formato indicado por OpenAI (para GPT-3.5 Turbo), pues vamos a hacer fine-tuning a un modelos de ellos:\n",
        "\n",
        "* https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates\n",
        "* https://platform.openai.com/docs/guides/fine-tuning\n",
        "\n",
        "Es necesario instalar:\n",
        "\n",
        "* pip install pandas\n",
        "* pip install openai\n",
        "* pip install openai tenacity\n",
        "\n",
        "Utilicé la version 0.27.0 de openai (tenacity me daba problemas con la última versión)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import random\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai.api_key = \"...\" # Aquí debes colocar tu propia API key de OpenAI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Escribe tu **prompt** (debe ser muy descriptiva).\n",
        "\n",
        "Elige un valor para **temperature** (entre 0 y 1) que deseas usar al generar el dataset. Valores más bajos son ideales para tareas precisas, como escribir código, mientras que valores más altos son mejores para tareas creativas, como escribir historias.\n",
        "\n",
        "Elige un valor para **number_of_examples**. Cuantos más generes, más tiempo tomará y será más costosa la generación del dataset. En general, más ejemplos conducirán a un modelo de mayor calidad. Por lo general, se recomienda un mínimo de 100 para comenzar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R7WKZyxtpUPS"
      },
      "outputs": [],
      "source": [
        "prompt = \"A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.\"\n",
        "temperature = 0.8\n",
        "number_of_examples = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "En el repositorio ya están creados los ejemplos de entrenamiento para esta prueba (**training_example.jsonl**).\n",
        "\n",
        "Correr para generar el dataset. Esto gasta muchos créditos con GPT-4, cuidado.\n",
        "\n",
        "* https://openai.com/pricing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rdsd82ngpHCG"
      },
      "outputs": [],
      "source": [
        "N_RETRIES = 3\n",
        "\n",
        "@retry(stop = stop_after_attempt(N_RETRIES), wait = wait_exponential(multiplier = 1, min = 4, max = 70))\n",
        "\n",
        "def generate_example(prompt, prev_examples, temperature = 0.5):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    if len(prev_examples) > 0:\n",
        "        if len(prev_examples) > 8:\n",
        "            prev_examples = random.sample(prev_examples, 8)\n",
        "        for example in prev_examples:\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": example\n",
        "            })\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model = \"gpt-4\",\n",
        "        messages = messages,\n",
        "        temperature = temperature,\n",
        "        max_tokens = 1000,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "# Generar ejemplos:\n",
        "prev_examples = []\n",
        "for i in range(number_of_examples):\n",
        "    print(f\"Generating example {i}\")\n",
        "    example = generate_example(prompt, prev_examples, temperature)\n",
        "    prev_examples.append(example)\n",
        "\n",
        "print(prev_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC6iJzXjugJ-"
      },
      "source": [
        "Tambien necesitamos generar un mensaje del sistema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMcfhW6Guh2E"
      },
      "outputs": [],
      "source": [
        "def generate_system_message(prompt):\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model = \"gpt-4\",\n",
        "      messages = [\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": prompt.strip(),\n",
        "        }\n",
        "      ],\n",
        "      temperature = temperature,\n",
        "      max_tokens = 500,\n",
        "  )\n",
        "\n",
        "  return response.choices[0].message[\"content\"]\n",
        "\n",
        "system_message = generate_system_message(prompt)\n",
        "\n",
        "print(f\"The system message is: `{system_message}` Feel free to re-run this cell if you want a better result.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6BqZ-hjseBF"
      },
      "source": [
        "Ahora, se colocan los ejemplos en un DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CEdkYeRsdmB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Crear listas vacias para almacenar prompts y las respuestas:\n",
        "prompts = []\n",
        "responses = []\n",
        "\n",
        "# Analizar prompts y las respuestas de los ejemplos:\n",
        "for example in prev_examples:\n",
        "    try:\n",
        "        split_example = example.split(\"-----------\")\n",
        "        prompts.append(split_example[1].strip())\n",
        "        responses.append(split_example[3].strip())\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Crear un DataFrame.\n",
        "df = pd.DataFrame({\n",
        "    \"prompt\": prompts,\n",
        "    \"response\": responses\n",
        "})\n",
        "\n",
        "# Para guardar el DataFrame en un archivo CSV:\n",
        "# df.to_csv('training_examples.csv', index = False)\n",
        "\n",
        "# Remover duplicados:\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "print(\"There are \" + str(len(df)) + \" successfully-generated examples.\")\n",
        "\n",
        "# Crear una lista vacía para almacenar ejemplos de entrenamiento:\n",
        "training_examples = []\n",
        "\n",
        "# Crea ejemplos de entrenamiento en el formato necesario para el fine-tuning de GPT-3.5 Turbo:\n",
        "for index, row in df.iterrows():\n",
        "    training_example = {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_message.strip()},\n",
        "            {\"role\": \"user\", \"content\": row['prompt']},\n",
        "            {\"role\": \"assistant\", \"content\": row['response']}\n",
        "        ]\n",
        "    }\n",
        "    training_examples.append(training_example)\n",
        "\n",
        "# Guardar ejemplos de entrenamiento en un archivo JSONL:\n",
        "with open(\"training_examples.jsonl\", \"w\") as f:\n",
        "    for example in training_examples:\n",
        "        f.write(json.dumps(example) + '\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
